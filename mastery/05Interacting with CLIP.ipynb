{"cells":[{"cell_type":"markdown","metadata":{"id":"YPHN7PJgKOzb"},"source":["# Interacting with CLIP\n","\n","이번 실습에서는 CLIP 모델을 다운로드하고 실행하며, 이미지와 텍스트 입력 간의 유사성을 계산하여, zero-shot 이미지 분류를 수행하는 방법을 실행해봅니다."]},{"cell_type":"markdown","metadata":{"id":"53N4k0pj_9qL"},"source":["# Preparation for Colab\n","\n","GPU 런타임을 실행 중인지 확인하세요. 그렇지 않다면 메뉴에서 Runtime > Change Runtime Type으로 가서 하드웨어 가속기를 \"GPU\"로 선택하세요. 다음 셀은 clip 패키지와 그 종속 항목들을 설치하고, PyTorch 1.7.1 이상 버전이 설치되어 있는지 확인할 것입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0BpdJkdBssk9"},"outputs":[],"source":["! pip install ftfy regex tqdm\n","! pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C1hkDT38hSaP"},"outputs":[],"source":["import numpy as np\n","import torch\n","from pkg_resources import packaging\n","\n","print(\"Torch version:\", torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"eFxgLV5HAEEw"},"source":["# Loading the model\n","\n","`clip.available_models()`는 사용 가능한 CLIP 모델의 이름을 나열합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLFS29hnhlY4"},"outputs":[],"source":["import clip\n","\n","clip.available_models()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBRVTY9lbGm8"},"outputs":[],"source":["model, preprocess = clip.load(\"ViT-B/32\")\n","model.cuda().eval()\n","input_resolution = model.visual.input_resolution\n","context_length = model.context_length\n","vocab_size = model.vocab_size\n","\n","print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n","print(\"Input resolution:\", input_resolution)\n","print(\"Context length:\", context_length)\n","print(\"Vocab size:\", vocab_size)"]},{"cell_type":"markdown","metadata":{"id":"21slhZGCqANb"},"source":["# Image Preprocessing\n","\n","모델이 예상하는 이미지로 전처리하기 위해, 데이터셋의 평균과 표준 편차를 사용하여 픽셀 강도를 정규화합니다. 이후 모델 입력 해상도에 맞추기 위해 크기를 조정하고 center-crop합니다.\n","\n","clip.load()의 반환 값에는 이러한 전처리를 수행하는 torchvision의 Transform을 포함합니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6cpiIFHp9N6"},"outputs":[],"source":["preprocess"]},{"cell_type":"markdown","metadata":{"id":"xwSB5jZki3Cj"},"source":["# Text Preprocessing\n","\n","우리는 대소문자를 구분하지 않는 토크나이저를 사용하며, 이는 `clip.tokenize()`를 사용하여 호출할 수 있습니다. 기본적으로 출력은 CLIP 모델이 기대하는 77개 토큰 길이로 패딩됩니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGom156-i2kL"},"outputs":[],"source":["clip.tokenize(\"Hello World!\")"]},{"cell_type":"markdown","metadata":{"id":"4W8ARJVqBJXs"},"source":["# Setting up input images and texts\n","\n","8개의 예제 이미지와 그에 대한 텍스트 설명을 모델에 입력하고, 해당하는 특징들 간의 유사성을 비교할 것입니다.\n","\n","토크나이저는 대소문자를 구분하지 않으며, 우리는 자유롭게 텍스트 설명을 제공할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMc1AXzBlhzm"},"outputs":[],"source":["import os\n","import skimage\n","import IPython.display\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import numpy as np\n","\n","from collections import OrderedDict\n","import torch\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","# images in skimage to use and their textual descriptions\n","descriptions = {\n","    \"page\": \"a page of text about segmentation\",\n","    \"chelsea\": \"a facial photo of a tabby cat\",\n","    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n","    \"rocket\": \"a rocket standing on a launchpad\",\n","    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n","    \"camera\": \"a person looking at a camera on a tripod\",\n","    \"horse\": \"a black-and-white silhouette of a horse\",\n","    \"coffee\": \"a cup of coffee on a saucer\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NSSrLY185jSf"},"outputs":[],"source":["original_images = []\n","images = []\n","texts = []\n","plt.figure(figsize=(16, 5))\n","\n","data_dir = \"/usr/local/lib/python3.10/dist-packages/skimage/data\"\n","\n","for filename in [filename for filename in os.listdir(data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n","    name = os.path.splitext(filename)[0]\n","    if name not in descriptions:\n","        continue\n","\n","    image = Image.open(os.path.join(data_dir, filename)).convert(\"RGB\")\n","\n","    plt.subplot(2, 4, len(images) + 1)\n","    plt.imshow(image)\n","    plt.title(f\"{filename}\\n{descriptions[name]}\")\n","    plt.xticks([])\n","    plt.yticks([])\n","\n","    original_images.append(image)\n","    images.append(preprocess(image))\n","    texts.append(descriptions[name])\n","\n","plt.tight_layout()\n"]},{"cell_type":"markdown","metadata":{"id":"WEVKsji6WOIX"},"source":["## Building features\n","\n","우리는 이미지를 정규화하고, 각 텍스트 입력을 토큰화한 다음, 모델의 순방향 패스를 실행하여 이미지와 텍스트 특징을 얻습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HBgCanxi8JKw"},"outputs":[],"source":["image_input = torch.tensor(np.stack(images)).cuda()\n","text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZN9I0nIBZ_vW"},"outputs":[],"source":["with torch.no_grad():\n","    image_features = model.encode_image(image_input).float()\n","    text_features = model.encode_text(text_tokens).float()"]},{"cell_type":"markdown","metadata":{"id":"cuxm2Gt4Wvzt"},"source":["## Calculating cosine similarity\n","\n","우리는 특징을 정규화하고 각 쌍의 내적을 계산합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yKAxkQR7bf3A"},"outputs":[],"source":["image_features /= image_features.norm(dim=-1, keepdim=True)\n","text_features /= text_features.norm(dim=-1, keepdim=True)\n","similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"C5zvMxh8cU6m"},"outputs":[],"source":["count = len(descriptions)\n","\n","plt.figure(figsize=(20, 14))\n","plt.imshow(similarity, vmin=0.1, vmax=0.3)\n","# plt.colorbar()\n","plt.yticks(range(count), texts, fontsize=18)\n","plt.xticks([])\n","for i, image in enumerate(original_images):\n","    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n","for x in range(similarity.shape[1]):\n","    for y in range(similarity.shape[0]):\n","        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n","\n","for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n","  plt.gca().spines[side].set_visible(False)\n","\n","plt.xlim([-0.5, count - 0.5])\n","plt.ylim([count + 0.5, -2])\n","\n","plt.title(\"Cosine similarity between text and image features\", size=20)"]},{"cell_type":"markdown","metadata":{"id":"alePijoXy6AH"},"source":["# 실습과제 Zero-Shot Image Classification 구현하기\n","\n","이 과제는 CLIP 모델을 활용하여 CIFAR-100 이미지 데이터셋에 대한 zero-shot classification을 구현하는 과제입니다. Zero-shot classification은 모델이 학습 과정에서 본 적 없는 새로운 클래스를 분류할 수 있는 능력을 의미합니다. CLIP은 이미지와 텍스트 사이의 일반적인 시맨틱 정보를 학습하여, 주어진 텍스트 설명과 가장 잘 맞는 이미지를 찾아내는 데 사용됩니다."]},{"cell_type":"markdown","metadata":{"id":"xXBjliwxT3P2"},"source":["텍스트 설명 준비: CIFAR-100의 각 클래스에 해당하는 텍스트 설명을 준비합니다. 예를 들어, 클래스 이름이 \"apple\"이면, 해당 텍스트 설명은 \"This is a photo of a apple\"이 됩니다. 이런 식으로 모든 클래스에 대한 설명을 생성합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nqu4GlfPfr-p"},"outputs":[],"source":["from torchvision.datasets import CIFAR100\n","\n","cifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4S__zCGy2MT"},"outputs":[],"source":["text_descriptions = [f\"This is a photo of a {label}\" for label in cifar100.classes]\n","text_tokens = clip.tokenize(text_descriptions).cuda()"]},{"cell_type":"markdown","metadata":{"id":"IE8gHyuOT7ss"},"source":["이제 다음 단계를 직접 구현할 차례입니다:\n","\n","텍스트 특성 추출: 토큰화된 텍스트를 CLIP 모델을 통해 인코딩하고, 각 텍스트 설명의 특성을 추출합니다.\n","\n","이미지 특성 추출: CIFAR-100 데이터셋의 이미지를 전처리하고, CLIP 모델을 사용하여 이미지에서 특성을 추출합니다.\n","\n","이미지-텍스트 매칭: 이미지 특성과 텍스트 특성 간의 유사도를 계산하여, 각 이미지에 가장 잘 맞는 텍스트 레이블(클래스)을 찾습니다.\n","\n","결과 출력: 사진에 대해 예측된 클래스와 그 확률을 출력합니다.(가능하다면 top 5 까지의 class의 확률을 출력합니다.)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T6Ju_6IBE2Iz"},"outputs":[],"source":["#코드를 작성하시오.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb","timestamp":1680559368390}]},"gpuClass":"standard","kernelspec":{"display_name":"ai_py309","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
